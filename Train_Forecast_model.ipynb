{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41543167",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39785204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "\n",
    "import numpy as np\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "from os import makedirs\n",
    "from abc import abstractmethod\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torch.nn.functional import sigmoid\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import deque\n",
    "\n",
    "from pyphoon2.DigitalTyphoonDataset import DigitalTyphoonDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ab7372",
   "metadata": {},
   "source": [
    "Define LSTM class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcc25a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LSTM : forward()\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size,#input_size is the number of expected features of the input\n",
    "                            hidden_size,#hidden_size is the number of features in the hidden state\n",
    "                            num_layers,#num_layers is the number of recurrent layers, This means stacking LSTMs and taking the outputs of the first LSTM and computing the final results.\n",
    "                            batch_first=True,#Batch first means that the inputs/outputs are provided as (batch,seq, feature), instead of (seq,batch,feature)\n",
    "                            dropout=0.1)#Dropout is an additional layer on top of the outputs of the lstm onto the next layer. Has a probabilty of skipping the next layer via dropout\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, output_size),\n",
    "        ) # This turns a sequence of modules into one larger module, the inputs of one goes into the next and so on\n",
    "    #LSTM\n",
    "    #Input_size :  69\n",
    "    #Hidden_size : 1024\n",
    "    #num_layers : 3\n",
    "    #Output_size :  8\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        out, _ = self.lstm(x, (h0, c0)) # Creates a series for predictions (t+9,t+12,....,t+36)\n",
    "\n",
    "        out = self.fc(out[:, -1, :]).squeeze() # Take only the last prediction t+36\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a752a03e",
   "metadata": {},
   "source": [
    "Define a base trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2378cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define BaseTrainer: train(), _save_checkpoint(), load_checkpoint()\n",
    "class BaseTrainer:\n",
    "    def __init__(self,\n",
    "                 train_loader,\n",
    "                 val_loader,\n",
    "                 args) -> None:\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.device = args.device\n",
    "        self.batch_size = args.batch_size\n",
    "        self.lr = args.lr\n",
    "        self.num_epochs = args.num_epochs\n",
    "        self.num_workers = args.num_workers\n",
    "    \n",
    "        self.log_interval = args.log_interval\n",
    "        self.save_interval = args.save_interval\n",
    "        self.save_dir = f\"{args.save_dir}/{args.experiment}/{args.run_name}\"\n",
    "        makedirs(self.save_dir, exist_ok=True)\n",
    "        self.opt = torch.optim.AdamW(self.model.parameters(), lr=self.lr)\n",
    "        self.lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(self.opt, T_max=self.num_epochs)\n",
    "        if args.checkpoint is not None:\n",
    "            self._load_checkpoint(args.checkpoint)\n",
    "        else:\n",
    "            self.model_params = list(self.model.parameters())\n",
    "            self.name = args.run_name\n",
    "            self.step = 0\n",
    "            self.epoch = 0\n",
    "\n",
    "        print(f\"Model has {sum(p.numel() for p in self.model_params):,} parameters and trainer is ready\")\n",
    "        print(f\"Weights will be saved at {self.save_interval} intervals\")\n",
    "\n",
    "    def train(self):\n",
    "        train_epochs = range(self.epoch, self.num_epochs)\n",
    "        for _ in train_epochs:\n",
    "            #Here is the actual training\n",
    "            self._run_train_epoch()\n",
    "            #Save at each interval\n",
    "            if self.epoch % self.save_interval == 0:\n",
    "                self._save_checkpoint()\n",
    "            # Store validation loss\n",
    "            val_loss = self._run_val_epoch()\n",
    "            self.epoch += 1\n",
    "\n",
    "        self._save_checkpoint()\n",
    "\n",
    "    @abstractmethod\n",
    "    def _run_train_epoch(self):\n",
    "        ...\n",
    "    @abstractmethod\n",
    "    def _run_val_epoch(self):\n",
    "        ...\n",
    "    \n",
    "    def _save_checkpoint(self, name=None):\n",
    "        model_dict = self.model.state_dict()\n",
    "\n",
    "        data = dict(\n",
    "            model_dict=model_dict,\n",
    "            opt_dict=self.opt.state_dict(),\n",
    "            epoch=self.epoch,\n",
    "            step=self.step,\n",
    "            name=self.name,\n",
    "        )\n",
    "\n",
    "        path = f\"{self.save_dir}/checkpoint_{self.step if name is None else name}.pth\"\n",
    "\n",
    "        torch.save(data, path)\n",
    "        print(f\"Checkpoint saved in {path}\")\n",
    "\n",
    "\n",
    "    def _load_checkpoint(self, path):\n",
    "        data = torch.load(path)\n",
    "        self.model.load_state_dict(data[\"model_dict\"])\n",
    "        self.model_params = list(self.model.parameters())\n",
    "        self.opt.load_state_dict(data[\"opt_dict\"])\n",
    "        self.epoch = data[\"epoch\"]\n",
    "        self.step = data[\"step\"]\n",
    "        self.name = f\"{data['name']}_resumed\"\n",
    "\n",
    "        print(\"=\"*100)\n",
    "        print(f\"Resuming training from checkpoint {path}\")\n",
    "        print(\"=\"*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e6c85f",
   "metadata": {},
   "source": [
    "Variables to be defined by user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bd96261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables to be defined by user\n",
    "\n",
    "#### =========================================== Variables that need defining by user,\n",
    "\n",
    "## Path to preprocessed sequences \n",
    "_path_to_data='F:/Data folder for ML/AU/AU'\n",
    "\n",
    "## Path to preprocessed sequences \n",
    "#_preprocessed_path='C:/Users/nilss/Desktop/Advanded ML FOLDer/benchmarks/r34p_10k_w6'\n",
    "_preprocessed_path='C:/Users/nilss/Desktop/Advanded ML FOLDer/outputs-Typhoon_prediction/r34p_10k_w6'\n",
    "\n",
    "## Path to where to save the model weights \n",
    "_save_dir='C:/Users/nilss/Desktop/Advanded ML FOLDer/models'\n",
    "\n",
    "#### =========================================== \n",
    "\n",
    "## Path to previous checkpoints weights, if set to None no check\n",
    "#_checkpoint='C:/Users/nilss/Desktop/Advanded ML FOLDer/models/ts/lstm_10kp_3l_1024_3i_pressure/checkpoint_30000.pth'\n",
    "_checkpoint='C:/Users/nilss/Desktop/Advanded ML FOLDer/outputs-Typhoon_prediction/models/ts/lstm_10kp_3l_1024_3i_pressure/checkpoint_3.pth'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c02087c",
   "metadata": {},
   "source": [
    "Parameters and arguments in a parser object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89dcae89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters and arguments\n",
    "\n",
    "_labels_input=\"0,1,2,3,4\"\n",
    "_labels_output=\"3,4\" # Solution to temporary problem\n",
    "_out_dim=len([int(x) for x in _labels_output.split(\",\")])\n",
    "\n",
    "# ========== Config ==========\n",
    "parser = ArgumentParser()\n",
    "parser.add_argument(\"-c\", \"--config_file\", type=str)\n",
    "parser.add_argument(\"--experiment\", type=str, default=\"ts\")\n",
    "parser.add_argument(\"--run_name\", type=str, default=f\"lstm_{int(time())}\")\n",
    "\n",
    "\n",
    "# ========== Dataset / Data =========\n",
    "parser.add_argument(\"--path_to_data\", type=str, default=_path_to_data)\n",
    "parser.add_argument(\"--preprocessed_path\", type=str, default=_preprocessed_path)\n",
    "parser.add_argument(\"--save_dir\", type=str, default=_save_dir)\n",
    "parser.add_argument(\"--checkpoint\", type=str, default=_checkpoint)\n",
    "parser.add_argument(\"--labels\", type=str, default=\"pressure\")\n",
    "parser.add_argument(\"--labels_input\", type=str, default=list(map(int, _labels_input.split(\",\"))))\n",
    "parser.add_argument(\"--labels_output\", type=str, default=list(map(int, _labels_output.split(\",\"))))\n",
    "parser.add_argument(\"--interval\", type=int, default=3)\n",
    "parser.add_argument(\"--use_date\", type=bool, default=False)\n",
    "parser.add_argument(\"--pred_diff\", type=bool, default=False)\n",
    "\n",
    "# ========== Model ==========\n",
    "parser.add_argument(\"--ts_model\", type=str, default=\"lstm\")\n",
    "parser.add_argument(\"--backbone\", type=str)\n",
    "parser.add_argument(\"--dim\", type=int)\n",
    "parser.add_argument(\"--hidden_dim\", type=int, default=1024)\n",
    "parser.add_argument(\"--out_dim\", type=int,default=_out_dim)\n",
    "parser.add_argument(\"--num_layers\", type=int, default=3)\n",
    "\n",
    "# ========== Time Series Params ==========\n",
    "parser.add_argument(\"--num_inputs\", type=int, default=3)\n",
    "parser.add_argument(\"--num_outputs\", type=int, default=8)\n",
    "\n",
    "# ========== Training ==========\n",
    "parser.add_argument(\"--lr\", type=float, default=2e-4)\n",
    "parser.add_argument(\"--batch_size\", type=int, default=128)\n",
    "parser.add_argument(\"--num_epochs\", type=int, default=100000)\n",
    "parser.add_argument(\"--weight_decay\", type=float, default=0)\n",
    "parser.add_argument(\"--es_patience\", type=int, default=-1)\n",
    "parser.add_argument(\"--log_interval\", type=int, default=10)\n",
    "parser.add_argument(\"--save_interval\", type=int, default=1000)\n",
    "\n",
    "parser.add_argument(\"--seed\", type=int, default=42)\n",
    "\n",
    "# ========== MoCo ==========\n",
    "parser.add_argument(\"--queue_size\", type=int, default=65536)\n",
    "parser.add_argument(\"--temperature\", type=float, default=0.07)\n",
    "\n",
    "# ========== Scheduler ==========\n",
    "parser.add_argument(\"--ws_range\", type=str, default=\"\")\n",
    "parser.add_argument(\"--ws_warmup\", type=int)\n",
    "parser.add_argument(\"--ws_last\", type=int)\n",
    "\n",
    "# ========== System ==========\n",
    "parser.add_argument(\"--device\", type=str, default= torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "parser.add_argument(\"--num_workers\", type=int, default=0) #Number of workers used while training. Default is set to 0 to prevent errors with incompatible systems\n",
    "\n",
    "parser = parser.parse_args([])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc795cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LABEL_SIZE, NORMALIZATION\n",
    "LABEL_SIZE = dict(\n",
    "    month=12,\n",
    "    day=31,\n",
    "    hour=24,\n",
    "    # grade=6,\n",
    ")\n",
    "\n",
    "NORMALIZATION = dict(\n",
    "    pressure=[983.8, 22.5],\n",
    "    wind=[36.7, 32.7],\n",
    "    lat=[22.58, 10.6],\n",
    "    lng=[136.2, 17.3],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe73427a",
   "metadata": {},
   "source": [
    "Define a time series data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba887d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define TimeSeriesTrainer(BaseTrainer): _run_train_epoch, _run_val_epoch\n",
    "class TimeSeriesTrainer(BaseTrainer):\n",
    "    def __init__(self, train_loader, val_loader, args) -> None:\n",
    "        if args.ts_model == \"lstm\":\n",
    "            self.model = LSTM(\n",
    "                train_loader.dataset.dataset.get_input_size(),\n",
    "                hidden_size=args.hidden_dim,\n",
    "                num_layers=args.num_layers,\n",
    "                output_size=train_loader.dataset.dataset.num_preds\n",
    "            ).to(args.device)\n",
    "        print(\"LSTM :\")\n",
    "        print(\" - Input_size : \",train_loader.dataset.dataset.get_input_size())\n",
    "        print(\" - Hidden_size : \", args.hidden_dim)\n",
    "        print(\" - num_layers : \", args.num_layers)\n",
    "        print(\" - Output_size : \", train_loader.dataset.dataset.num_preds)\n",
    "        super().__init__(train_loader, val_loader, args)\n",
    "\n",
    "\n",
    "\n",
    "        self.reg_criterion = nn.MSELoss()\n",
    "        self.labels = args.labels\n",
    "\n",
    "\n",
    "    def _run_train_epoch(self):\n",
    "        self.model.train()\n",
    "        pbar = tqdm(self.train_loader, desc=f\"Training {self.epoch+1}/{self.num_epochs}\")\n",
    "        losses = dict(loss=deque(maxlen=self.log_interval))\n",
    "\n",
    "        for batch in pbar:\n",
    "            \n",
    "            self.opt.zero_grad()\n",
    "            inp, outs = batch[0].to(self.device), batch[1].to(self.device)\n",
    "            preds = self.model(inp)\n",
    "\n",
    "            if \"grade\" in self.labels:\n",
    "                outs = outs == 6\n",
    "                preds = sigmoid(preds)\n",
    "            outs = outs[:, :, 0]\n",
    "            loss = self.reg_criterion(preds, outs.float())\n",
    "            #loss = self.reg_criterion(preds, (outs).float().squeeze())\n",
    "            losses[\"loss\"].append(loss.item())\n",
    "\n",
    "            loss.backward()\n",
    "            self.opt.step()\n",
    "\n",
    "            self.step += 1\n",
    "\n",
    "            avg = {f\"tr_{key}\": np.mean(val) for key, val in losses.items()}\n",
    "            pbar.set_postfix(dict(loss=loss.item(), **avg))\n",
    "\n",
    "\n",
    "\n",
    "        self.lr_scheduler.step()\n",
    "\n",
    "    def _run_val_epoch(self):\n",
    "        self.model.eval()\n",
    "        pbar = tqdm(self.val_loader, desc=f\"Eval {self.epoch+1}/{self.num_epochs}\")\n",
    "        losses = dict(loss=list())\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in pbar:\n",
    "                inp, outs = batch[0].to(self.device), batch[1].to(self.device)\n",
    "                preds = self.model(inp)\n",
    "\n",
    "                if \"grade\" in self.labels:\n",
    "                    outs = outs == 6\n",
    "                    preds = torch.sigmoid(preds)\n",
    "\n",
    "                outs = outs[:, :, 0]  # selecting only the first output variable (e.g. pressure)\n",
    "                loss = self.reg_criterion(preds, outs.float())\n",
    "                losses[\"loss\"].append(loss.item())\n",
    "\n",
    "                avg = {f\"val_{key}\": np.mean(val) for key, val in losses.items()}\n",
    "                pbar.set_postfix(dict(loss=loss.item(), **avg))\n",
    "\n",
    "\n",
    "\n",
    "        return np.mean(losses[\"loss\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21e1fb5",
   "metadata": {},
   "source": [
    "Define a fisheye class used to transform images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72bacdab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define FishEye : fisheye_grid(), forward()\n",
    "def fisheye_grid(width, height, alpha):\n",
    "    # Create a grid of normalized coordinates\n",
    "    x, y = torch.meshgrid(torch.linspace(-1, 1, width), torch.linspace(-1, 1, height), indexing=\"ij\")\n",
    "    coords = torch.stack((y, x), dim=-1)\n",
    "\n",
    "    # Apply fisheye transformation to the coordinates\n",
    "    r = torch.sqrt(coords[:, :, 0]**2 + coords[:, :, 1]**2)\n",
    "    radial_scale = torch.pow(r, alpha)#(1 - torch.pow(r, alpha)) / r\n",
    "    radial_scale[r == 0] = 1.0\n",
    "    fisheye_coords = coords * torch.unsqueeze(radial_scale, -1)\n",
    "\n",
    "    # Clamp the transformed coordinates to [-1, 1] range\n",
    "    fisheye_coords = torch.clamp(fisheye_coords, min=-1, max=1)\n",
    "\n",
    "    return fisheye_coords\n",
    "\n",
    "class FishEye(torch.nn.Module):\n",
    "    def __init__(self, size, alpha):\n",
    "        super().__init__()\n",
    "        self.grid = fisheye_grid(size, size, alpha)\n",
    "\n",
    "    def forward(self, img):\n",
    "        if len(img.shape) == 3:\n",
    "            img = img.unsqueeze(0)\n",
    "\n",
    "        B, _, _, _ = img.shape\n",
    "        fish = F.grid_sample(img, self.grid.unsqueeze(0).repeat(B, 1, 1, 1), align_corners=True).squeeze(0)\n",
    "        return fish"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a80ca29",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ðŸ“–Dataloader </summary>\n",
    "<a name=\"dataloader\"></a>\n",
    "**Transform and filter functions **\n",
    "- These functions aim to zoom in on the more important parts of the images and filter out images not deemed fit\n",
    "\n",
    "**Data loading  **\n",
    "- get_TS_dataloader(args) is defined for future use\n",
    "- Data is split into training, validation and testing and then loaded\n",
    "\n",
    "\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c40fefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    FishEye(256, 0.2),\n",
    "])\n",
    "def transform_func(obj, trans=transforms):\n",
    "    \n",
    "    img_range = [150, 350]\n",
    "    img, labels = obj\n",
    "    img = img.clip(img_range[0], img_range[1])\n",
    "    img = (img - img_range[0])/(img_range[1]-img_range[0])\n",
    "    y, m, d, h = labels\n",
    "    label = datetime(year=y, month=m, day=d, hour=h)\n",
    "    return trans(img.astype(np.float32)), label\n",
    "\n",
    "if \"grade\" in parser.labels:\n",
    "    def filter_func(x):\n",
    "        return x.grade() < 8\n",
    "elif \"pressure\" in parser.labels:\n",
    "    def filter_func(x):\n",
    "        return x.grade() < 8\n",
    "    \n",
    "def get_TS_dataloader(args):\n",
    "\n",
    "    dataset = STD(labels=[\"month\", \"day\", \"hour\", \"pressure\", \"wind\"],\n",
    "                preprocessed_path=args.preprocessed_path,\n",
    "                latent_dim= args.out_dim,\n",
    "                x=          args.labels_input,#args.labels_input,#[0,1,2,3,4],\n",
    "                y=          args.labels_output,#args.labels_output,#[3,4],\n",
    "                num_inputs= args.num_inputs,\n",
    "                num_preds=  args.num_outputs,\n",
    "                interval=   args.interval,\n",
    "                filter_func=filter_func,\n",
    "                prefix =    args.path_to_data,\n",
    "                pred_diff=  args.pred_diff,\n",
    "                )\n",
    "\n",
    "    train, val, test = dataset.random_split([0.7, 0.15, 0.15], split_by=\"sequence\")\n",
    "    print(\"Number of sequences in data split.\")\n",
    "    print(f\"Training-Validation-Test :\\n{len(train)}-\\n{len(val))}-\\n{len(test)}\")\n",
    "\n",
    "\n",
    "    train_loader = DataLoader(train,\n",
    "                              batch_size=args.batch_size,\n",
    "                              shuffle=True,\n",
    "                              num_workers=args.num_workers)\n",
    "    val_loader = DataLoader(val,\n",
    "                            batch_size=args.batch_size,\n",
    "                            shuffle=False,\n",
    "                            num_workers=args.num_workers)\n",
    "    test_loader = DataLoader(test,\n",
    "                            batch_size=args.batch_size,\n",
    "                            shuffle=False,\n",
    "                            num_workers=args.num_workers)\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68094ff",
   "metadata": {},
   "source": [
    "Define a dataset loader (includes preprocessed images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3845339b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#class STD(DigitalTyphoonDataset): filter_sequences(sequence), __getitem__(self, idx),get_input_size,get_output_size, _labels_from_label_strs(self, image, label_strs),_prepare_labels, get_sequence_images, get_sequence\n",
    "class STD(DigitalTyphoonDataset):\n",
    "    # arguments to the following function is still hard coded\n",
    "    def __init__(self,\n",
    "                 labels,\n",
    "                 x,\n",
    "                 y,\n",
    "                 num_inputs=6,\n",
    "                 num_preds=20,\n",
    "                 interval=1,\n",
    "                 #output_all=False,\n",
    "                 preprocessed_path=None,\n",
    "                 latent_dim=None,\n",
    "                 pred_diff=False,\n",
    "                 prefix=r\"F:\\Data folder for ML\\AU\\AU\",\n",
    "                 spectrum=\"Infrared\",\n",
    "                 load_data_into_memory=False,\n",
    "                 ignore_list=None,\n",
    "                 filter_func=None,\n",
    "                 transform_func=None,\n",
    "                 transform=None,\n",
    "                 verbose=False) -> None:\n",
    "        \"\"\"\n",
    "        labels: labels to include in [\"year\", \"month\", \"day\", \"hour\", \"grade\", \"lat\", \"lng\", \"pressure\", \"wind\"]\n",
    "        include_images: boolean to include or not images when generating sequences\n",
    "        x: sequence data to use as inputs. should be array indices corresponding to the order in which labels are requested\n",
    "        y: sequence data to use as targets. should be array indices corresponding to the order in which labels are requested\n",
    "            images, if included are always included\n",
    "        num_inputs: length of sequence used as input to the model\n",
    "        num_preds: length of predicted datapoints\n",
    "        preprocess: preprocess images to a smaller feature vector\n",
    "        \"\"\"\n",
    "        super().__init__(f\"{prefix}/image/\",\n",
    "                        f\"{prefix}/metadata/\",\n",
    "                        f\"{prefix}/metadata.json\",\n",
    "                         labels,\n",
    "                         \"sequence\",\n",
    "                         spectrum,\n",
    "                         True,\n",
    "                         load_data_into_memory,\n",
    "                         ignore_list,\n",
    "                         filter_func,\n",
    "                         transform_func,\n",
    "                         transform,\n",
    "                         verbose)\n",
    "        idx = 0\n",
    "        self.x = []\n",
    "        self.y = []\n",
    "\n",
    "        for i, label in enumerate(labels):\n",
    "            sz = LABEL_SIZE[label] if label in LABEL_SIZE else 1\n",
    "            if i in x:\n",
    "                self.x.extend(list(range(idx, idx+sz)))\n",
    "            if i in y:\n",
    "                self.y.extend(list(range(idx, idx+sz)))\n",
    "            idx += sz\n",
    "\n",
    "        if preprocessed_path is None:\n",
    "            print(\"Error : The preprocessed_path was set None or not defined properly\")\n",
    "        else:\n",
    "            assert latent_dim is not None\n",
    "            if -1 in self.x:\n",
    "                self.x.extend(list(range(idx, idx+latent_dim)))\n",
    "            if -1 in self.y:\n",
    "                self.y.extend(list(range(idx, idx+latent_dim)))\n",
    "        def get_input_size(self):\n",
    "            return len(self.x)\n",
    "\n",
    "        def get_output_size(self):\n",
    "            return len(self.y)\n",
    "\n",
    "        self.preprocessed_path = preprocessed_path\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_preds = num_preds\n",
    "        self.interval = interval\n",
    "        #self.output_all = output_all\n",
    "        self.pred_diff = pred_diff\n",
    "        self.slice_inputs = lambda start_idx: slice(start_idx, start_idx+(self.num_inputs*self.interval),self.interval)\n",
    "        self.slice_outputs = lambda start_idx: slice(start_idx+(self.num_inputs*self.interval),start_idx+((self.num_inputs+self.num_preds)*self.interval), self.interval)\n",
    "\n",
    "        def filter_sequences(sequence):\n",
    "            # Remove sequences which are smaller than the prediction period or if they are too old, this was found out by trial and error\n",
    "            if sequence.get_num_images() < (self.num_inputs + self.num_preds)*self.interval+1 or sequence.images[0].year() < 1987:\n",
    "                return True\n",
    "\n",
    "\n",
    "        for seq in self.sequences:\n",
    "            if filter_sequences(seq): # If the image is not acceptible, choose to not use that image\n",
    "                self.number_of_images -= seq.get_num_images() \n",
    "                seq.images.clear()\n",
    "                self.number_of_nonempty_sequences -= 1\n",
    "\n",
    "        self.number_of_nonempty_sequences += 1\n",
    "\n",
    "    def get_input_size(self):\n",
    "        return len(self.x)\n",
    "    def get_output_size(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Gather images from a sequence and label them in a torch stack\n",
    "        seq = self.get_ith_sequence(idx)\n",
    "        images = seq.get_all_images_in_sequence()\n",
    "        labels = torch.stack([self._labels_from_label_strs(image, self.labels) for image in images])\n",
    "\n",
    "        if self.preprocessed_path is not None:\n",
    "            # The images in the preprocessed folder are npz images with keys 'arr_1' and 'arr_2'\n",
    "            npz = np.load(f\"{self.preprocessed_path}/{seq.sequence_str}.npz\")\n",
    "            names_to_features = dict(zip(npz[\"arr_1\"], npz[\"arr_0\"]))\n",
    "            features = [names_to_features[str(img.image_filepath).split(\"/\")[-1].split(\".\")[0]]\n",
    "                        for img in images]\n",
    "            features = torch.from_numpy(np.array(features))\n",
    "\n",
    "            labels = torch.cat((labels, features), dim=1)\n",
    "\n",
    "        #if self.output_all:\n",
    "        #    return labels, seq.sequence_str\n",
    "\n",
    "        start_idx = np.random.randint(0, seq.get_num_images()-(self.num_inputs + self.num_preds)*self.interval)\n",
    "        lab_inputs = labels[self.slice_inputs(start_idx), self.x]\n",
    "        lab_preds = labels[self.slice_outputs(start_idx), self.y]\n",
    "\n",
    "        if self.pred_diff:\n",
    "            lab_preds = lab_preds - labels[self.slice_inputs(start_idx), self.y][-1]\n",
    "\n",
    "        return lab_inputs, lab_preds\n",
    "\n",
    "\n",
    "\n",
    "    def _labels_from_label_strs(self, image, label_strs):\n",
    "        \"\"\"\n",
    "        Given an image and the label/labels to retrieve from the image, returns a single label or\n",
    "        a list of labels\n",
    "\n",
    "        :param image: image to access labels for\n",
    "        :param label_strs: either a List of label strings or a single label string\n",
    "        :return: a List of label strings or a single label string\n",
    "        \"\"\"\n",
    "        if isinstance(label_strs, list) or isinstance(label_strs, tuple):\n",
    "            label_ray = torch.cat([self._prepare_labels(image.value_from_string(label), label) for label in label_strs])\n",
    "            return label_ray\n",
    "        else:\n",
    "            label = self._prepare_labels(image.value_from_string(label_strs), label_strs)\n",
    "            return label\n",
    "\n",
    "    def _prepare_labels(self, value, label):\n",
    "        if label in LABEL_SIZE:\n",
    "            one_hot = torch.zeros(LABEL_SIZE[label])\n",
    "            if label == \"hour\":\n",
    "                one_hot[value] = 1\n",
    "            elif label == \"grade\":\n",
    "                one_hot[value-2] = 1\n",
    "            else:\n",
    "                one_hot[value-1] = 1\n",
    "            return one_hot\n",
    "        else:\n",
    "            # Normalize\n",
    "            if label in NORMALIZATION:\n",
    "\n",
    "                mean, std = NORMALIZATION[label]\n",
    "                return (torch.Tensor([value]) - mean) / std\n",
    "\n",
    "            if label == \"grade\":\n",
    "                return torch.Tensor([float(value)])\n",
    "\n",
    "            return torch.Tensor([value])\n",
    "\n",
    "    def get_sequence_images(self, seq_str):\n",
    "        def crop(img, cropx=224, cropy=224):\n",
    "            y,x = img.shape\n",
    "            startx = x//2-(cropx//2)\n",
    "            starty = y//2-(cropy//2)\n",
    "            return img[starty:starty+cropy,startx:startx+cropx]\n",
    "        idx = self._sequence_str_to_seq_idx[seq_str]\n",
    "        seq = self.sequences[idx]\n",
    "        images = seq.get_all_images_in_sequence()\n",
    "        return [crop(image.image()) for image in images]\n",
    "\n",
    "    def get_sequence(self, seq_str):\n",
    "        idx = self._sequence_str_to_seq_idx[seq_str]\n",
    "        return self.__getitem__(idx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22e729d",
   "metadata": {},
   "source": [
    "Load dataset and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e07572f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "269 train sequences\n",
      "58 val sequences\n",
      "57 test sequences\n",
      "LSTM\n",
      "Input_size :  69\n",
      "Hidden_size :  1024\n",
      "num_layers :  3\n",
      "Output_size :  8\n",
      "====================================================================================================\n",
      "Resuming training from checkpoint C:/Users/nilss/Desktop/Advanded ML FOLDer/outputs-Typhoon_prediction/models/ts/lstm_10kp_3l_1024_3i_pressure/checkpoint_3.pth\n",
      "====================================================================================================\n",
      "Model has 21,807,624 parameters and trainer is ready\n",
      "Weights will be saved at 1000 intervals\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 1/100000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:09<00:00,  3.20s/it, loss=0.781, tr_loss=10.7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved in C:/Users/nilss/Desktop/Advanded ML FOLDer/models/ts/lstm_1749224750/checkpoint_6.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval 1/100000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.07s/it, loss=0.827, val_loss=0.827]\n",
      "Training 2/100000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:06<00:00,  2.26s/it, loss=147, tr_loss=54.3]   \n",
      "Eval 2/100000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.38s/it, loss=0.513, val_loss=0.513]\n",
      "Training 3/100000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:06<00:00,  2.32s/it, loss=0.531, tr_loss=10.4]\n",
      "Eval 3/100000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.33s/it, loss=0.406, val_loss=0.406]\n",
      "Training 4/100000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:07<00:00,  2.36s/it, loss=0.462, tr_loss=10.2]\n",
      "Eval 4/100000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.51s/it, loss=0.495, val_loss=0.495]\n",
      "Training 5/100000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:06<00:00,  2.31s/it, loss=0.622, tr_loss=9.83]\n",
      "Eval 5/100000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.42s/it, loss=0.43, val_loss=0.43]\n",
      "Training 6/100000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:06<00:00,  2.25s/it, loss=0.756, tr_loss=9.16]\n",
      "Eval 6/100000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.51s/it, loss=0.307, val_loss=0.307]\n",
      "Training 7/100000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:06<00:00,  2.23s/it, loss=0.348, tr_loss=8.16]\n",
      "Eval 7/100000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.42s/it, loss=0.334, val_loss=0.334]\n",
      "Training 8/100000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:06<00:00,  2.20s/it, loss=0.39, tr_loss=7.32]\n",
      "Eval 8/100000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.30s/it, loss=0.43, val_loss=0.43]\n",
      "Training 9/100000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:06<00:00,  2.17s/it, loss=0.61, tr_loss=6.68]\n",
      "Eval 9/100000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.39s/it, loss=0.406, val_loss=0.406]\n",
      "Training 10/100000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:06<00:00,  2.15s/it, loss=155, tr_loss=52.1]   \n",
      "Eval 10/100000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.48s/it, loss=0.39, val_loss=0.39]\n",
      "Training 11/100000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:06<00:00,  2.20s/it, loss=0.468, tr_loss=5.61]\n",
      "Eval 11/100000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.37s/it, loss=0.185, val_loss=0.185]\n",
      "Training 12/100000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:06<00:00,  2.24s/it, loss=0.53, tr_loss=4.99]\n",
      "Eval 12/100000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.25s/it, loss=0.266, val_loss=0.266]\n",
      "Training 13/100000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:06<00:00,  2.10s/it, loss=60.7, tr_loss=22.6] \n",
      "Eval 13/100000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.21s/it, loss=0.403, val_loss=0.403]\n",
      "Training 14/100000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:06<00:00,  2.09s/it, loss=0.498, tr_loss=4.19] \n",
      "Eval 14/100000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.48s/it, loss=0.403, val_loss=0.403]\n",
      "Training 15/100000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:06<00:00,  2.17s/it, loss=48.8, tr_loss=18.3] \n",
      "Eval 15/100000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.24s/it, loss=0.138, val_loss=0.138]\n",
      "Training 16/100000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:06<00:00,  2.10s/it, loss=42.3, tr_loss=15.9] \n",
      "Eval 16/100000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.36s/it, loss=0.351, val_loss=0.351]\n",
      "Training 17/100000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:06<00:00,  2.13s/it, loss=0.438, tr_loss=2.94]\n",
      "Eval 17/100000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.31s/it, loss=0.154, val_loss=0.154]\n",
      "Training 18/100000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:06<00:00,  2.25s/it, loss=0.317, tr_loss=2.54]\n",
      "Eval 18/100000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.32s/it, loss=0.183, val_loss=0.183]\n",
      "Training 19/100000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:06<00:00,  2.09s/it, loss=0.133, tr_loss=2.17]\n",
      "Eval 19/100000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.28s/it, loss=0.198, val_loss=0.198]\n",
      "Training 20/100000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:06<00:00,  2.11s/it, loss=0.243, tr_loss=1.91]\n",
      "Eval 20/100000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.22s/it, loss=0.23, val_loss=0.23]\n",
      "Training 21/100000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:06<00:00,  2.19s/it, loss=0.158, tr_loss=1.62] \n",
      "Eval 21/100000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.37s/it, loss=0.366, val_loss=0.366]\n",
      "Training 22/100000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:06<00:00,  2.21s/it, loss=0.397, tr_loss=1.54]\n",
      "Eval 22/100000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.15s/it, loss=0.252, val_loss=0.252]\n",
      "Training 23/100000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:05<00:00,  1.99s/it, loss=0.233, tr_loss=1.33] \n",
      "Eval 23/100000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.25s/it, loss=0.333, val_loss=0.333]\n",
      "Training 24/100000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:06<00:00,  2.11s/it, loss=0.209, tr_loss=1.19]\n",
      "Eval 24/100000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.27s/it, loss=0.233, val_loss=0.233]\n",
      "Training 25/100000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:06<00:00,  2.08s/it, loss=0.395, tr_loss=1.11] \n",
      "Eval 25/100000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.25s/it, loss=0.191, val_loss=0.191]\n",
      "Training 26/100000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:06<00:00,  2.02s/it, loss=0.326, tr_loss=1.02] \n",
      "Eval 26/100000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.22s/it, loss=0.195, val_loss=0.195]\n",
      "Training 27/100000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:06<00:00,  2.10s/it, loss=0.134, tr_loss=0.843]\n",
      "Eval 27/100000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.21s/it, loss=0.265, val_loss=0.265]\n",
      "Training 28/100000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:05<00:00,  1.95s/it, loss=7.91, tr_loss=3.12]  \n",
      "Eval 28/100000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.25s/it, loss=0.222, val_loss=0.222]\n",
      "Training 29/100000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:07<00:00,  2.34s/it, loss=0.27, tr_loss=0.706] \n",
      "Eval 29/100000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.47s/it, loss=0.145, val_loss=0.145]\n",
      "Training 30/100000:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:04<00:09,  4.68s/it, loss=0.289, tr_loss=0.289]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load sequences and train model\u001b[39;00m\n\u001b[0;32m      2\u001b[0m train_loader, val_loader, _ \u001b[38;5;241m=\u001b[39m get_TS_dataloader(parser)\n\u001b[1;32m----> 3\u001b[0m \u001b[43mTimeSeriesTrainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mparser\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 36\u001b[0m, in \u001b[0;36mBaseTrainer.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     33\u001b[0m train_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_epochs)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m train_epochs:\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;66;03m#Here is the actual training\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_train_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;66;03m#Save at each interval\u001b[39;00m\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[1;32mIn[7], line 35\u001b[0m, in \u001b[0;36mTimeSeriesTrainer._run_train_epoch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     32\u001b[0m pbar \u001b[38;5;241m=\u001b[39m tqdm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     33\u001b[0m losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(loss\u001b[38;5;241m=\u001b[39mdeque(maxlen\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_interval))\n\u001b[1;32m---> 35\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpbar\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43minp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mouts\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nilss\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[0;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[0;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[1;32mc:\\Users\\nilss\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    714\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\nilss\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    763\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 764\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    766\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\nilss\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:50\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[1;32m---> 50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\nilss\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:420\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[1;34m(self, indices)\u001b[0m\n\u001b[0;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "Cell \u001b[1;32mIn[10], line 104\u001b[0m, in \u001b[0;36mSTD.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    102\u001b[0m seq \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_ith_sequence(idx)\n\u001b[0;32m    103\u001b[0m images \u001b[38;5;241m=\u001b[39m seq\u001b[38;5;241m.\u001b[39mget_all_images_in_sequence()\n\u001b[1;32m--> 104\u001b[0m labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_labels_from_label_strs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images])\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocessed_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;66;03m# The images in the preprocessed folder are npz images with keys 'arr_1' and 'arr_2'\u001b[39;00m\n\u001b[0;32m    108\u001b[0m     npz \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocessed_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseq\u001b[38;5;241m.\u001b[39msequence_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.npz\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[10], line 140\u001b[0m, in \u001b[0;36mSTD._labels_from_label_strs\u001b[1;34m(self, image, label_strs)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;124;03mGiven an image and the label/labels to retrieve from the image, returns a single label or\u001b[39;00m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;124;03ma list of labels\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;124;03m:return: a List of label strings or a single label string\u001b[39;00m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(label_strs, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(label_strs, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 140\u001b[0m     label_ray \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue_from_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlabel_strs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m label_ray\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load sequences and train model\n",
    "train_loader, val_loader, _ = get_TS_dataloader(parser)\n",
    "TimeSeriesTrainer(train_loader,val_loader,parser).train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
